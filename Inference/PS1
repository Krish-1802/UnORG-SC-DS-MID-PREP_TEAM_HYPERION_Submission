{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aad8da7f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-17T23:40:58.901247Z",
     "iopub.status.busy": "2025-04-17T23:40:58.900986Z",
     "iopub.status.idle": "2025-04-17T23:41:14.020088Z",
     "shell.execute_reply": "2025-04-17T23:41:14.019282Z"
    },
    "papermill": {
     "duration": 15.124234,
     "end_time": "2025-04-17T23:41:14.021789",
     "exception": false,
     "start_time": "2025-04-17T23:40:58.897555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 23:41:02.208435: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744933262.408387      13 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744933262.467781      13 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/cluster2model/tensorflow2/default/1/cluster2.weights.h5\n",
      "/kaggle/input/ps-dataset/order_data_last_six_month.xlsx - Worksheet.csv\n",
      "/kaggle/input/ps-dataset/associated_order_item_data_last_six_month.xlsx - Worksheet.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "from  tqdm import  tqdm\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import matplotlib.pyplot as plt \n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a65ffe2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T23:41:14.027480Z",
     "iopub.status.busy": "2025-04-17T23:41:14.026963Z",
     "iopub.status.idle": "2025-04-17T23:41:14.031305Z",
     "shell.execute_reply": "2025-04-17T23:41:14.030441Z"
    },
    "papermill": {
     "duration": 0.008355,
     "end_time": "2025-04-17T23:41:14.032625",
     "exception": false,
     "start_time": "2025-04-17T23:41:14.024270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "link='/kaggle/input/ps-dataset/order_data_last_six_month.xlsx - Worksheet.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c0b2b13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T23:41:14.037006Z",
     "iopub.status.busy": "2025-04-17T23:41:14.036779Z",
     "iopub.status.idle": "2025-04-17T23:41:14.042181Z",
     "shell.execute_reply": "2025-04-17T23:41:14.040834Z"
    },
    "papermill": {
     "duration": 0.009047,
     "end_time": "2025-04-17T23:41:14.043636",
     "exception": false,
     "start_time": "2025-04-17T23:41:14.034589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fill_missing_predictions(all_customer_ids, pred_df, num_days=14):\n",
    "    \"\"\"\n",
    "    Fills missing customers in the prediction dataframe with 0s for all days.\n",
    "    \n",
    "    Parameters:\n",
    "    - all_customer_ids: list or array of all customer IDs you expect\n",
    "    - pred_df: DataFrame with predictions (must have 'customer_id' as first column)\n",
    "    - num_days: number of prediction days (default=14)\n",
    "    \n",
    "    Returns:\n",
    "    - A DataFrame with one row per customer, and prediction columns from 0 to num_days-1\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure column names are correct\n",
    "    day_cols = list(range(num_days))\n",
    "    \n",
    "    # Create an empty prediction DataFrame with 0s\n",
    "    base_df = pd.DataFrame({\n",
    "        'customer_id': all_customer_ids\n",
    "    })\n",
    "    \n",
    "    for i in day_cols:\n",
    "        base_df[i] = 0.0\n",
    "\n",
    "    # Merge actual predictions over the base (will overwrite 0s for existing IDs)\n",
    "    final_df = base_df.set_index('customer_id').combine_first(\n",
    "        pred_df.set_index('customer_id')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Ensure column order: customer_id, 0, 1, ..., 13\n",
    "    final_df = final_df[['customer_id'] + day_cols]\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "651830e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T23:41:14.048238Z",
     "iopub.status.busy": "2025-04-17T23:41:14.047994Z",
     "iopub.status.idle": "2025-04-17T23:41:14.060050Z",
     "shell.execute_reply": "2025-04-17T23:41:14.059389Z"
    },
    "papermill": {
     "duration": 0.015984,
     "end_time": "2025-04-17T23:41:14.061630",
     "exception": false,
     "start_time": "2025-04-17T23:41:14.045646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference(link):\n",
    "    SEQ_LEN = 30\n",
    "    PRED_HORIZON = 14\n",
    "    df=pd.read_csv(link)\n",
    "    all_customers=df['customer_id'].unique()\n",
    "    df['order_date'] = pd.to_datetime(df['order_date'],dayfirst=True)\n",
    "\n",
    "    max_date = df['order_date'].max()\n",
    "    customer_behavior=pd.DataFrame(df['customer_id'].unique(),columns=['customer_id'])\n",
    "    \n",
    "    for idx in df['customer_id'].unique():\n",
    "        customer_data = df[df['customer_id'] == idx]\n",
    "    \n",
    "        total_orders = customer_data['order_id'].nunique()\n",
    "        total_order_days = customer_data['order_date'].nunique()\n",
    "    \n",
    "        average_orders_placed = total_orders / total_order_days if total_order_days > 0 else 0\n",
    "        total_order_value = customer_data['net_order_amount'].sum()\n",
    "        average_order_value = total_order_value / total_orders if total_orders > 0 else 0\n",
    "\n",
    "    # Calculate average order gap days\n",
    "        order_dates = customer_data['order_date'].sort_values()\n",
    "        order_gaps = order_dates.diff().dt.days.dropna()\n",
    "        avg_order_gap_days = order_gaps.mean() if not order_gaps.empty else 192\n",
    "\n",
    "    # Order consistency score (1 / std of gaps)\n",
    "    # Assign to customer_behavior DataFrame\n",
    "        customer_behavior.loc[customer_behavior['customer_id'] == idx, 'Total Orders'] = total_orders\n",
    "        customer_behavior.loc[customer_behavior['customer_id'] == idx, 'Average Order Gap Days'] = avg_order_gap_days\n",
    "    zero_people=customer_behavior[customer_behavior['Total Orders']<=4]['customer_id']\n",
    "    cluster_1_people=customer_behavior[((customer_behavior['Total Orders']<=25) & (customer_behavior['Total Orders']>4) & (customer_behavior['Average Order Gap Days']>=14))]['customer_id']\n",
    "    cluster_2_people = customer_behavior['customer_id'][~customer_behavior['customer_id'].isin(cluster_1_people) & ~customer_behavior['customer_id'].isin(zero_people)]\n",
    "\n",
    "    train_data=df[df['customer_id'].isin(cluster_2_people)]\n",
    "    train_data.drop(['order_number','customer_name','poc_name','poc_id','amount','profit','order_status','warehouse_id'],axis=1,inplace=True)\n",
    "    df = train_data.copy()\n",
    "    df['order_date'] = pd.to_datetime(df['order_date'], dayfirst=True)\n",
    "\n",
    "# group & aggregate:\n",
    "    daily_df = df.groupby(['customer_id','order_date']).agg(\n",
    "        daily_order_count = ('order_id',       'count'),\n",
    "        discount          = ('discount',       'sum'),\n",
    "        net_order_amount  = ('net_order_amount','sum'),\n",
    "        warehouse_name    = ('warehouse_name', 'first')  # or use mode if you prefer\n",
    "    ).reset_index()\n",
    "    daily_df_dummy=pd.get_dummies(daily_df,columns=['warehouse_name'],dtype='int')\n",
    "    df = daily_df_dummy.copy()\n",
    "    df['order_date'] = pd.to_datetime(df['order_date'])\n",
    "\n",
    "# Add 'day_of_week' column\n",
    "    df['day_of_week'] = df['order_date'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "\n",
    "# One-hot encode 'day_of_week'\n",
    "    df = pd.get_dummies(df, columns=['day_of_week'], prefix='dow',dtype='int')\n",
    "\n",
    "\n",
    "\n",
    "#---------#\n",
    "\n",
    "    df = df[df['order_date'] >= max_date - pd.Timedelta(days=29)]\n",
    "\n",
    "    pred_cust_id=df['customer_id'].unique()\n",
    "# Identify feature columns (exclude ID/date)\n",
    "    feature_cols = [c for c in df.columns \n",
    "                    if c not in ('customer_id', 'order_date')]\n",
    "\n",
    "# Containers for sequence data\n",
    "    X = []\n",
    "\n",
    "    for cust_id, group in tqdm(df.groupby('customer_id'), total=df['customer_id'].nunique()):\n",
    "        group = group.set_index('order_date').sort_index()\n",
    "        full_idx = pd.date_range(group.index.min(), group.index.max(), freq='D')\n",
    "        group = group.reindex(full_idx).fillna(0)\n",
    "\n",
    "        group['customer_id'] = cust_id\n",
    "        group['date'] = group.index\n",
    "\n",
    "        data = group[feature_cols].values\n",
    "\n",
    "        if len(data) >= SEQ_LEN:\n",
    "            X.append(data[-SEQ_LEN:, :])  # only last 30 days\n",
    "        else:\n",
    "        # pad at beginning with zeros if less than 30\n",
    "            padded = np.zeros((SEQ_LEN, data.shape[1]))\n",
    "            padded[-len(data):, :] = data\n",
    "            X.append(padded)\n",
    "\n",
    "    X = np.stack(X)\n",
    "    NUM_FEATURES = len(feature_cols)\n",
    "    model_best = Sequential([\n",
    "    # 1st LSTM layer, returns sequences so we can stack another LSTM\n",
    "        LSTM(128, input_shape=(SEQ_LEN, NUM_FEATURES), return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "\n",
    "    # 2nd LSTM layer\n",
    "        LSTM(64, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "\n",
    "    # Dense “bottleneck” to learn combined features\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.1),\n",
    "\n",
    "    # Final output: 14 sigmoid neurons, one per future day\n",
    "        Dense(PRED_HORIZON, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "\n",
    "# Important: you need to compile it again\n",
    "    model_best.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# Load the best saved weights\n",
    "    model_best.load_weights('/kaggle/input/cluster2model/tensorflow2/default/1/cluster2.weights.h5')\n",
    "    pred=model_best.predict(X)\n",
    "    #pred_bin= (pred > 0.5).astype(int)   \n",
    "    prob=pd.concat([pd.Series(pred_cust_id,name='customer_id'),pd.DataFrame(pred)],axis=1)\n",
    "    return fill_missing_predictions(all_customers,prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "135aa611",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T23:41:14.066859Z",
     "iopub.status.busy": "2025-04-17T23:41:14.066576Z",
     "iopub.status.idle": "2025-04-17T23:41:23.824558Z",
     "shell.execute_reply": "2025-04-17T23:41:23.823825Z"
    },
    "papermill": {
     "duration": 9.761811,
     "end_time": "2025-04-17T23:41:23.825725",
     "exception": false,
     "start_time": "2025-04-17T23:41:14.063914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/3101840981.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data.drop(['order_number','customer_name','poc_name','poc_id','amount','profit','order_status','warehouse_id'],axis=1,inplace=True)\n",
      "100%|██████████| 1565/1565 [00:02<00:00, 698.05it/s]\n",
      "2025-04-17 23:41:22.289884: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 22 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n"
     ]
    }
   ],
   "source": [
    "pred=inference(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b91b7f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T23:41:23.834885Z",
     "iopub.status.busy": "2025-04-17T23:41:23.834599Z",
     "iopub.status.idle": "2025-04-17T23:41:23.870041Z",
     "shell.execute_reply": "2025-04-17T23:41:23.869238Z"
    },
    "papermill": {
     "duration": 0.041183,
     "end_time": "2025-04-17T23:41:23.871296",
     "exception": false,
     "start_time": "2025-04-17T23:41:23.830113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred.to_csv(\"final_14_day_order_proba.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0792a314",
   "metadata": {
    "papermill": {
     "duration": 0.002865,
     "end_time": "2025-04-17T23:41:23.878631",
     "exception": false,
     "start_time": "2025-04-17T23:41:23.875766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7141798,
     "sourceId": 11402349,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 307832,
     "modelInstanceId": 287014,
     "sourceId": 343179,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 31.925997,
   "end_time": "2025-04-17T23:41:26.984720",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-17T23:40:55.058723",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
